{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "presidential-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alien-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is imported\n",
    "df = pandas.read_csv(\"train.csv\")\n",
    "#irrelavent data is removed\n",
    "df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "#NaNs are removed\n",
    "df = df.dropna()\n",
    "#testing and training sets are split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-camcorder",
   "metadata": {},
   "source": [
    "entropy $H(Y)$, conditional-entropy $H(Y|X)$, information gain $IG(X)$\n",
    "\n",
    "$H(Y) = - \\sum^K_{i=1} P(Y=y_i)\\lg P(Y=y_i)$\n",
    "\n",
    "$H(Y|X) = - \\sum^V_{j=1} \\biggl( P(X=x_j) \\sum^K_{i=1} P(Y=y_i|X=x_j)\\lg P(Y=y_i|X=x_j) \\biggr)$\n",
    "\n",
    "$IG(X) = H(Y)-H(Y|X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cleared-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a value and returns the product of that and it's base 2 log\n",
    "#I use this function to make the following functions more readable\n",
    "def helper(a):\n",
    "    if (a == 1) or (a == 0):\n",
    "        return 0\n",
    "    return (a * np.log2(a))\n",
    "#this function calculates the entropy(H(Y)) of the dataframe dataset\n",
    "#classname will be 'Survive'\n",
    "#the PosClassPrior/NegClassPrior will represent the prior probability of the positive/negative class\n",
    "def entropy(dataset, classname):\n",
    "    PosClassPrior = len(dataset[dataset[classname] == 1]) / len(dataset)\n",
    "    NegClassPrior = 1 - PosClassPrior\n",
    "    return 0 - (helper(PosClassPrior) + helper(NegClassPrior))\n",
    "#this function calculates the conditional-entropy(H(Y|X)) of the dataframe dataset\n",
    "#predictors are a list of strings. each string contains the conditional that the feature = some value\n",
    "def conditionalEntropy(dataset, classname, predictors):\n",
    "    #total is the variable we store the conditional entropy during the summations\n",
    "    total = 0.0\n",
    "    #a temporary dataframe holding a subset of our data\n",
    "    tempDF = None\n",
    "    #these 3 variables represent the predictor prior probability P(X=x), and \n",
    "    #the positive and negative class conditional densities P(Y=y|X=x)\n",
    "    predPrior = 0.0\n",
    "    PosClassCond = 0.0\n",
    "    NegClassCond = 0.0\n",
    "    #I iterate through every feature in the dataset\n",
    "    for predictor in predictors:\n",
    "        #here the subset of the data where feature = value is stored in tempDF\n",
    "        #this if statement avoids a division by zero error\n",
    "        tempDF = dataset.query(predictor)\n",
    "        if(len(tempDF) > 0):\n",
    "            #here the values are set using the afformentioned variables according to the formula\n",
    "            predPrior = len(tempDF) / len(dataset)\n",
    "            PosClassCond = len(tempDF[tempDF['Survived'] == 1]) / len(tempDF)\n",
    "            NegClassCond = len(tempDF[tempDF['Survived'] != 1]) / len(tempDF)\n",
    "            #and it is added to the total before iterating to the next value in \n",
    "            #the outer summation of the conditional entropy formula\n",
    "            total -= predPrior * (helper(NegClassCond) + helper(PosClassCond))\n",
    "    return total\n",
    "#here the info gain is calculated and rounded to the tenth decimal place\n",
    "#I round because any difference less than that is negligable and I want to save space\n",
    "def infoGain(dataset, classname, predictors):\n",
    "    a = entropy(dataset, classname) - conditionalEntropy(dataset, classname, predictors)\n",
    "    return round(a,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-pioneer",
   "metadata": {},
   "source": [
    "Gini Index $I_G(p)$. Best-Case 1 or 0, worst-case $\\frac 1 2$.\n",
    "\n",
    "$p_+ = \\frac{\\text{items in the set}\\in C_+}{\\text{items in the set}}$\n",
    "\n",
    "$I_G(p) = 1 - \\sum^K_{j=1} p_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "committed-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the gini index\n",
    "def giniIndex(dataset):\n",
    "    #this if statement avoids error but should never occur\n",
    "    if len(dataset) == 0:\n",
    "        return 0.5\n",
    "    #positive and negatice represent the class prior probabilties\n",
    "    positive = len(dataset[dataset['Survived'] > 0]) / len(dataset)\n",
    "    negative = 1.0 - positive\n",
    "    #k=2 so this is the whole gini index summation and formula\n",
    "    return 1.0 - ((negative)**2 + (positive)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exterior-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function gathers all the query strings for each feature in the data and their set of values\n",
    "def getAllQs(data):\n",
    "    #Querys is a list of lists of strings\n",
    "    ##each list contains the conditionals for all unique values(strings) of that list's feature\n",
    "    #prev contains the previous value for certain features so they can be treated as a range if the testing data\n",
    "    #contains a possible value for such features not in the training data\n",
    "    Querys = []\n",
    "    prev = 0.0\n",
    "    #this iterates through every column/feature in the data/dataframe except for the target column\n",
    "    for col in data.columns[1:]:\n",
    "        Querys.append([])\n",
    "        prev = -0.1\n",
    "        #removes duplicates of each value for the feature in the samples\n",
    "        l=list(set(data[col]))\n",
    "        #sorts those values\n",
    "        l.sort()\n",
    "        #iterates through each unique value: v\n",
    "        for v in l:\n",
    "            #appends the feature to query the if statements are for syntax depending on the datatype of v\n",
    "            #they also update prev to be the current value after the proper range is copied for age/fare\n",
    "            if(col == 'Sex' or col == 'Embarked'):\n",
    "                Querys[-1].append('%s == \\'%s\\''%(col,v))\n",
    "            elif(col == 'Age' or col == 'Fare'):\n",
    "                Querys[-1].append('%s <= %f and %s > %f'%(col,v,col,prev))\n",
    "                prev = v\n",
    "            else:\n",
    "                Querys[-1].append('%s == %s'%(col,v))\n",
    "    return Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interim-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this functino returns a boolean indicating if all features match\n",
    "def all_features_match(datas):\n",
    "    #goes through each feature if the set of unique values has len > 1 there are non-matching features\n",
    "    for col in datas.columns[1:]:\n",
    "        if len(set(datas[col])) > 1:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-month",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "anonymous-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function builds our decision tree classifier as a list of lists\n",
    "#each list except for the \"root\" contains a conditional for the value of a feature and either the child nodes\n",
    "#or class prediction\n",
    "#max depth is a counter for recursion and gini indicates using gini index(=1) or information gain(=0)\n",
    "def BuildTree(datas, maxdepth, gini):\n",
    "    #checks if all target values belong to one class or if the features all match and returns the majority class label\n",
    "    if (0 == len(datas[datas['Survived'] == 1])):\n",
    "        return 0.0#a leafnode that predicts this\n",
    "    elif (len(datas) == len(datas[datas['Survived'] == 1])):\n",
    "        return 1.0#a leafnode that predicts this\n",
    "    if (all_features_match(datas) or maxdepth == 0):\n",
    "        return len(datas[datas['Survived'] == 1])/len(datas)#a leafnode that predicts this\n",
    "    #these 3 variables store the list of querries, the nd-array containing each feature's list of querries+index\n",
    "    #and a counter to keep track of the index within the for loop\n",
    "    Qs = getAllQs(datas)\n",
    "    store = np.zeros((len(Qs),2))\n",
    "    index = -1\n",
    "    #iterates through the values of Qs storing the gini-index or info-gain of each feature in store\n",
    "    for q in Qs:\n",
    "        index += 1\n",
    "        store[index][0] = index\n",
    "        if(gini == 1):\n",
    "            store[index][1] = giniIndex(datas)\n",
    "        else:\n",
    "            store[index][1] = infoGain(datas, 'Survived', q)\n",
    "            store[index][1] /= np.sqrt(len(q))#this is a penalty I added\n",
    "            #I added this penalty because the there is a bias favoring features with many unique values\n",
    "            #this becomes a problem because it is not necessarily the best split\n",
    "    #iterates through every row in store\n",
    "    for i,j in zip(store.T[0],store.T[1]):\n",
    "        #i is the index j is corresponding the GI or IG value\n",
    "        i = int(i)\n",
    "        #here we have found the highest value\n",
    "        if (j == max(set(store.T[1]))):\n",
    "            ##print(Qs[i][0])\n",
    "            child = []#the child nodes the len of this list = the number of unique values/ranges in the feature\n",
    "            for featureValue in Qs[i]:#iterates through each value for the feature\n",
    "                tempstr = featureValue.split(' ')[0]#this tempstr is the feature's name\n",
    "                tempdataf = datas.query(featureValue).drop(columns=tempstr)#this temp data is a subset of out data\n",
    "                #all values in tempdef have feature: tempstr equal to feature value\n",
    "                #the feature is then dropped because it's irrelavent now\n",
    "                child.append([featureValue, BuildTree(tempdataf, maxdepth-1, gini)])\n",
    "                #the function is recursively called with the subset as the data and max depth decremented\n",
    "                #finally the child is added to the tree this will happen for every unique feature value\n",
    "            return child\n",
    "    print(\"code should never reach here error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "peripheral-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the accuracy of my decision tree using a test dataframe\n",
    "def testTree(acc, test, tree):\n",
    "    tempData = None\n",
    "    #if tree depth=0\n",
    "    if(type(tree)==type(0.1)):\n",
    "        return len(test[test['Survived'] == tree])/len(test)\n",
    "    #otherwise it iterates through tree using the conditionals to seperate the test dataset by feature value\n",
    "    for leaf in tree:\n",
    "        #gets subset storing it in tempdata like in earlier functions\n",
    "        tempData = test.query(leaf[0])\n",
    "        if(len(tempData) == 0):\n",
    "            continue\n",
    "        elif(type(leaf[1]) == type(0.5)):#this means the leaf is a class prediction\n",
    "            ##print('%s %f'%(list(tempData['Survived']),leaf[1]))\n",
    "            if(leaf[1]>0.5):#accuracy is calculated and added\n",
    "                acc += len(tempData[tempData['Survived'] == 1])\n",
    "            else:\n",
    "                acc += len(tempData[tempData['Survived'] != 1])\n",
    "        else:#there are more branches that I use recursion to traverse\n",
    "            acc += testTree(0, tempData, leaf[1])\n",
    "    return acc#accuracy is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "partial-attendance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "0.8181818181818182\n",
      "0.8111888111888111\n"
     ]
    }
   ],
   "source": [
    "#here I print the length of the testing set call my functions and print my accuracy\n",
    "print(len(test))\n",
    "#This tree uses information gain with a max depth of 2\n",
    "mytree = BuildTree(train, 2, False)\n",
    "print(testTree(0, test, mytree)/len(test))\n",
    "#This tree uses gini index with a max depth of 2\n",
    "mytree2 = BuildTree(train, 2, True)\n",
    "print(testTree(0, test, mytree2)/len(test))\n",
    "\n",
    "#here I write my tree to a file not necessary for this assignment but helpful for visualization and debugging\n",
    "def printTree(tree, ver, branch, fi):\n",
    "    tempData = None\n",
    "    ind = ''\n",
    "    if(type(tree)==type(0.1)):\n",
    "        myfile.write(\"%f\\n\"%tree)\n",
    "        return tree\n",
    "    for leaf in tree:\n",
    "        ind = ''\n",
    "        tempData = ver.query(leaf[0])\n",
    "        if(type(leaf[1]) == type(0.5)):\n",
    "            for b in range(branch):\n",
    "                ind += '__'\n",
    "            myfile.write(\"%s%s\\n%s%s%s\\n\"%(ind, leaf[0], ind, ind, leaf[1]))\n",
    "        else:\n",
    "            for b in range(branch):\n",
    "                myfile.write('__')\n",
    "            myfile.write(\"%s,\\n\"%leaf[0])\n",
    "            printTree(leaf[1], tempData, branch+1, fi)\n",
    "myfile = open(\"./3tree.txt\", \"w\")\n",
    "try:\n",
    "    printTree(mytree2, train, 0, myfile)\n",
    "finally:\n",
    "    myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
