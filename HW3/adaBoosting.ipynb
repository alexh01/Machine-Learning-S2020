{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unknown-dictionary",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\\ \\epsilon_t=P_{i~U^{(t)}}[f_t(x_i)\\neq y_i]\n",
    "\\\\ \\text{train }f_2x\\text{ on new training set that fails }f_1x\\text{. found with }\\epsilon\n",
    "\\\\ \\epsilon_1 = \\frac {\\sum_n u_n^{(1)} \\delta (f_1(x_n)!=y_n)} {Z_1}\n",
    "\\\\\\text{where } Z_1 = \\sum_{n=1}^N u_n^{(1)},\\epsilon \\lt 0.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing like in 3\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "df = df.dropna()\n",
    "df = df.replace('female', 1).replace('male', 0).replace('C', 1).replace('Q', 2).replace('S', 3)\n",
    "#split the training and testing data\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "#initialize the decision tree\n",
    "dtc = DecisionTreeClassifier(max_depth=2, random_state=1, criterion='entropy')\n",
    "#set data and target equal to my input and expected output\n",
    "data = train.drop(columns=['Survived']).to_numpy()\n",
    "target = train['Survived'].to_numpy()\n",
    "#alter the expected output so it classifies the negative class as -1 instead of 0\n",
    "for t in range(len(target)):\n",
    "    if target[t] == 0:\n",
    "        target[t] = -1\n",
    "data2 = test.drop(columns=['Survived']).to_numpy()\n",
    "target2 = test['Survived'].to_numpy()\n",
    "#alter the expected output so it classifies the negative class as -1 instead of 0\n",
    "for t in range(len(target2)):\n",
    "    if target2[t] == 0:\n",
    "        target2[t] = -1\n",
    "#fits the tree to it's own training set\n",
    "dtc.fit(data, target)\n",
    "#prints the original accuracy\n",
    "print(dtc.score(data,target))\n",
    "print(dtc.score(data2,target2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates the current accuracy of the sum of the weighted models\n",
    "#Hi is the sum of the current votes for each sample(a list of len s)\n",
    "#tar is the expected target values\n",
    "#s is the length of the dataset\n",
    "def currAcc(Hi,tar,s):\n",
    "    finalAcc = 0\n",
    "    for j in range(s):\n",
    "        #because the sample classifications wont always be exactly -1/1\n",
    "        #I improvised this to treat the sign as the label\n",
    "        if (Hi[j] > 0 and tar[j] > 0) or (Hi[j] <= 0 and tar[j] <= 0):\n",
    "            finalAcc += 1\n",
    "    return finalAcc/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set N as the number of samples in train M for test\n",
    "N = len(train)\n",
    "M = len(test)\n",
    "#set u as the list of weight distributions for training\n",
    "u = np.zeros((1,N))\n",
    "for i in range(N):\n",
    "    u[0][i] = 1/N\n",
    "Z = 1.0\n",
    "#list of our lagrange multipliers and error rates\n",
    "a = []\n",
    "e = []\n",
    "alpha = 0.0\n",
    "epsilon = 0.0\n",
    "#variables for the final hypothesis H is where the votes are stored for each sample\n",
    "#and temp is each weighted models prediction for a sample\n",
    "temp = 0\n",
    "#initialize H and Htest\n",
    "H = [0]*N\n",
    "H2 = [0]*M\n",
    "#list of the test and train errors\n",
    "trainErrs = []\n",
    "testErrs = []\n",
    "#iterate for 500 rounds\n",
    "for R in range(1,501):\n",
    "    #set Z as the normalizer\n",
    "    Z = sum(u[0])\n",
    "    #here I initialize epsilon which will be added to the list of error rates\n",
    "    epsilon = 0.0\n",
    "    for sample in range(N):\n",
    "        epsilon += u[0][sample]*target[sample]*dtc.predict(data[sample].reshape(1, -1))\n",
    "    epsilon = 0.5 - epsilon/2\n",
    "    #here I initialize aplha which will be added to the list of error rates\n",
    "    alpha = 0.5*np.log((1-epsilon)/epsilon)\n",
    "    a.append(alpha)\n",
    "    e.append(epsilon)\n",
    "    #here I update the sample weights using the lagrange multiplier above as well as the latest weighted model\n",
    "    for sample in [range(N)]:\n",
    "        u[0][sample] *= np.exp(-alpha*target[sample]*dtc.predict(data[sample]))/Z\n",
    "    #here the model is refitted and the votes for the model are cast or added to the final hypothesis\n",
    "    dtc.fit(data, target, sample_weight=u[0])\n",
    "    #for train\n",
    "    for i in range(N):\n",
    "        temp = dtc.predict(data[i].reshape(1, -1))\n",
    "        H[i] += alpha*temp\n",
    "    #for test\n",
    "    for i in range(M):\n",
    "        H2[i] += alpha*dtc.predict(data2[i].reshape(1, -1))\n",
    "    #here I append the errors at this point to the lists using my helper function currAcc\n",
    "    trainErrs.append(1-currAcc(H,target,N))\n",
    "    testErrs.append(1-currAcc(H2,target2,M))\n",
    "    #dtc.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('error')\n",
    "plt.xlabel('rounds')\n",
    "plt.plot(trainErrs, label='training set')\n",
    "plt.plot(testErrs, label='testing set')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('final train acc %f'%(1-trainErrs[-1]))\n",
    "print('final test acc %f'%(1-testErrs[-1]))\n",
    "print('best train acc %f'%(1-min(trainErrs)))\n",
    "print('best test acc %f'%(1-min(testErrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-concord",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
